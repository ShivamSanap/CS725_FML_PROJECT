{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1wws47p-pCb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['axes.grid'] = True\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    plt.style.use('seaborn-darkgrid')\n",
        "\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adult_columns = [\n",
        "    'age','workclass','fnlwgt','education','education-num',\n",
        "    'marital-status','occupation','relationship','race','sex',\n",
        "    'capital-gain','capital-loss','hours-per-week','native-country','income'\n",
        "]\n",
        "\n",
        "DATA_PATH = '/content/adult.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    DATA_PATH,\n",
        "    names=adult_columns,\n",
        "    na_values=['?'],\n",
        "    skipinitialspace=True\n",
        ")\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "df['income'] = df['income'].str.strip()\n",
        "y = (df['income'] == '>50K').astype(int).to_numpy()\n",
        "X_df = df.drop(columns=['income'])\n",
        "\n",
        "categorical_cols = ['workclass','education','marital-status','occupation',\n",
        "                   'relationship','race','sex','native-country']\n",
        "numeric_cols = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
        "])\n",
        "\n",
        "X = preprocessor.fit_transform(X_df).astype(np.float64)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Positive rate (>50K): {y.mean():.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "l7nnS50TBdYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, n_features):\n",
        "        self.weights = np.zeros(n_features, dtype=np.float64)\n",
        "        self.bias = 0.0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.sigmoid(X @ self.weights + self.bias)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        eps = 1e-15\n",
        "        return -np.mean(y_true*np.log(y_pred+eps) + (1-y_true)*np.log(1-y_pred+eps))\n",
        "\n",
        "    def compute_gradients(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]\n",
        "        err = y_pred - y_true\n",
        "        dw = (X.T @ err)/m\n",
        "        db = np.mean(err)\n",
        "        return dw, db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_prob = self.forward(X)\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        return {\n",
        "            'loss': self.compute_loss(y_true, y_prob),\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_true, y_prob),\n",
        "        }"
      ],
      "metadata": {
        "id": "3JPSIqB-BdUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "    def update(self, w, b, dw, db):\n",
        "        return w - self.lr*dw, b - self.lr*db\n",
        "\n",
        "class MomentumOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.m = momentum\n",
        "        self.vw = None\n",
        "        self.vb = 0.0\n",
        "\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.vw is None:\n",
        "            self.vw = np.zeros_like(w)\n",
        "        self.vw = self.m * self.vw - self.lr * dw\n",
        "        self.vb = self.m * self.vb - self.lr * db\n",
        "        w = w + self.vw\n",
        "        b = b + self.vb\n",
        "        return w, b\n",
        "\n",
        "class NesterovOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.m = momentum\n",
        "        self.vw = None\n",
        "        self.vb = 0.0\n",
        "\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.vw is None:\n",
        "            self.vw = np.zeros_like(w)\n",
        "\n",
        "        vw_prev = self.vw.copy()\n",
        "        vb_prev = self.vb\n",
        "\n",
        "        self.vw = self.m * self.vw - self.lr * dw\n",
        "        self.vb = self.m * self.vb - self.lr * db\n",
        "\n",
        "        w = w + (-self.m * vw_prev + (1 + self.m) * self.vw)\n",
        "        b = b + (-self.m * vb_prev + (1 + self.m) * self.vb)\n",
        "\n",
        "        return w, b\n",
        "\n",
        "class AdagradOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.eps = epsilon\n",
        "        self.gs_w = None\n",
        "        self.gs_b = 0.0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.gs_w is None:\n",
        "            self.gs_w = np.zeros_like(w)\n",
        "        self.gs_w += dw**2\n",
        "        self.gs_b += db**2\n",
        "        return (w - self.lr*dw/(np.sqrt(self.gs_w)+self.eps),\n",
        "                b - self.lr*db/(np.sqrt(self.gs_b)+self.eps))\n",
        "\n",
        "class RMSpropOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, decay=0.9, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.decay = decay\n",
        "        self.eps = epsilon\n",
        "        self.ms_w = None\n",
        "        self.ms_b = 0.0\n",
        "\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.ms_w is None:\n",
        "            self.ms_w = np.zeros_like(w)\n",
        "        self.ms_w = self.decay * self.ms_w + (1 - self.decay) * (dw ** 2)\n",
        "        self.ms_b = self.decay * self.ms_b + (1 - self.decay) * (db ** 2)\n",
        "\n",
        "        w = w - self.lr * dw / (np.sqrt(self.ms_w) + self.eps)\n",
        "        b = b - self.lr * db / (np.sqrt(self.ms_b) + self.eps)\n",
        "        return w, b\n",
        "\n",
        "\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.b1 = beta1\n",
        "        self.b2 = beta2\n",
        "        self.eps = epsilon\n",
        "        self.mw = None\n",
        "        self.vw = None\n",
        "        self.mb = 0.0\n",
        "        self.vb = 0.0\n",
        "        self.t = 0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.mw is None:\n",
        "            self.mw = np.zeros_like(w)\n",
        "            self.vw = np.zeros_like(w)\n",
        "        self.t += 1\n",
        "        self.mw = self.b1*self.mw + (1-self.b1)*dw\n",
        "        self.mb = self.b1*self.mb + (1-self.b1)*db\n",
        "        self.vw = self.b2*self.vw + (1-self.b2)*(dw**2)\n",
        "        self.vb = self.b2*self.vb + (1-self.b2)*(db**2)\n",
        "        mw_hat = self.mw/(1 - self.b1**self.t)\n",
        "        mb_hat = self.mb/(1 - self.b1**self.t)\n",
        "        vw_hat = self.vw/(1 - self.b2**self.t)\n",
        "        vb_hat = self.vb/(1 - self.b2**self.t)\n",
        "        return (w - self.lr*mw_hat/(np.sqrt(vw_hat)+self.eps),\n",
        "                b - self.lr*mb_hat/(np.sqrt(vb_hat)+self.eps))"
      ],
      "metadata": {
        "id": "r25rCE2qBdSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 50\n",
        "    batch_size: int = 32\n",
        "    seed: int = 42\n",
        "    l2_lambda: float = 0.0\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def train_epoch(model, X_tr, y_tr, optimizer, cfg: TrainConfig, track_gradients=False):\n",
        "    n = X_tr.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    total_loss = 0.0\n",
        "    grad_norms = []\n",
        "    bs = cfg.batch_size\n",
        "\n",
        "    for i in range(0, n, bs):\n",
        "        xb = X_tr[idx[i:i+bs]]\n",
        "        yb = y_tr[idx[i:i+bs]]\n",
        "        yp = model.forward(xb)\n",
        "        base_loss = model.compute_loss(yb, yp)\n",
        "        l2_pen = 0.5 * cfg.l2_lambda * np.dot(model.weights, model.weights)\n",
        "        total_loss += (base_loss + l2_pen) * len(yb)\n",
        "\n",
        "        dw, db = model.compute_gradients(xb, yb, yp)\n",
        "        if cfg.l2_lambda > 0.0:\n",
        "            dw = dw + cfg.l2_lambda * model.weights\n",
        "\n",
        "        if track_gradients:\n",
        "            grad_norms.append(np.linalg.norm(dw))\n",
        "\n",
        "        model.weights, model.bias = optimizer.update(model.weights, model.bias, dw, db)\n",
        "\n",
        "    if track_gradients:\n",
        "        return total_loss / n, np.mean(grad_norms)\n",
        "    return total_loss / n\n",
        "\n",
        "def train_model_detailed(model, optimizer, X_tr, y_tr, X_va, y_va, cfg: TrainConfig):\n",
        "    set_seed(cfg.seed)\n",
        "    history = {\n",
        "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
        "        'val_acc': [], 'val_f1': [], 'val_roc_auc': [], 'grad_norm': []\n",
        "    }\n",
        "\n",
        "    for e in range(cfg.epochs):\n",
        "        tr_loss, grad_norm = train_epoch(model, X_tr, y_tr, optimizer, cfg, track_gradients=True)\n",
        "        val = model.evaluate(X_va, y_va)\n",
        "\n",
        "        history['epoch'].append(e+1)\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['val_loss'].append(val['loss'])\n",
        "        history['val_acc'].append(val['accuracy'])\n",
        "        history['val_f1'].append(val['f1'])\n",
        "        history['val_roc_auc'].append(val['roc_auc'])\n",
        "        history['grad_norm'].append(grad_norm)\n",
        "\n",
        "    return pd.DataFrame(history)"
      ],
      "metadata": {
        "id": "bKnsFfP7BdPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_optimizer(name, lr=0.01, momentum=0.9):\n",
        "    if name == 'SGD': return SGDOptimizer(lr)\n",
        "    if name == 'Momentum': return MomentumOptimizer(lr, momentum)\n",
        "    if name == 'Nesterov': return NesterovOptimizer(lr, momentum)\n",
        "    if name == 'Adagrad': return AdagradOptimizer(lr)\n",
        "    if name == 'RMSprop': return RMSpropOptimizer(lr)\n",
        "    if name == 'Adam': return AdamOptimizer(lr)\n",
        "    raise ValueError(name)\n",
        "\n",
        "BEST_HPARAMS = {\n",
        "    'Adam':     {'lr': 0.01,  'momentum': None},\n",
        "    'Momentum': {'lr': 0.01,  'momentum': 0.95},\n",
        "    'Nesterov': {'lr': 0.01,  'momentum': 0.95},\n",
        "    'RMSprop':  {'lr': 0.001, 'momentum': None},\n",
        "    'SGD':      {'lr': 0.1,   'momentum': None},\n",
        "    'Adagrad':  {'lr': 0.03,  'momentum': None},\n",
        "}\n",
        "\n",
        "OPTIMIZER_NAMES = ['Adam', 'Nesterov', 'Momentum', 'RMSprop', 'SGD', 'Adagrad']\n",
        "\n"
      ],
      "metadata": {
        "id": "tp-Shjn5BdNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convergence_histories = {}\n",
        "convergence_times = {}\n",
        "\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    print(f\"Training {name}...\")\n",
        "    n_features = X_train.shape[1]\n",
        "    lr = BEST_HPARAMS[name]['lr']\n",
        "    mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "    model = LogisticRegression(n_features)\n",
        "    opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "    cfg = TrainConfig(epochs=50, batch_size=32, seed=42)\n",
        "\n",
        "    start = pd.Timestamp.now()\n",
        "    hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "    elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
        "\n",
        "    convergence_histories[name] = hist_df\n",
        "    convergence_times[name] = elapsed\n",
        "\n",
        "    test_results = model.evaluate(X_test, y_test)\n",
        "    print(f\"  → F1 @ ep10: {hist_df.loc[9,'val_f1']:.4f}, ep30: {hist_df.loc[29,'val_f1']:.4f}, \"\n",
        "          f\"ep50: {hist_df.loc[49,'val_f1']:.4f} | Test F1: {test_results['f1']:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "colors = plt.cm.tab10(range(len(OPTIMIZER_NAMES)))\n",
        "\n",
        "ax = axes[0, 0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['train_loss'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Training Loss', fontsize=11)\n",
        "ax.set_title('Training Loss Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['val_loss'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Validation Loss', fontsize=11)\n",
        "ax.set_title('Validation Loss Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1, 0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['val_f1'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Validation F1 Score', fontsize=11)\n",
        "ax.set_title('Validation F1 Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1, 1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['grad_norm'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Gradient Norm', fontsize=11)\n",
        "ax.set_title('Gradient Stability', fontsize=12, fontweight='bold')\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "early_f1_gains = {\n",
        "    name: (hist.loc[9, 'val_f1'] - hist.loc[0, 'val_f1'])\n",
        "    for name, hist in convergence_histories.items()\n",
        "}\n",
        "\n",
        "stability = {\n",
        "    name: convergence_histories[name]['val_loss'].iloc[-20:].std()\n",
        "    for name in OPTIMIZER_NAMES\n",
        "}"
      ],
      "metadata": {
        "id": "wFCih8MgBdKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_auc_f1(history_df, max_epoch=50):\n",
        "\n",
        "    vals = history_df['val_f1'][:max_epoch].to_numpy()\n",
        "    return np.trapezoid(vals, dx=1)\n",
        "\n",
        "def find_convergence_epoch(history_df, metric='val_f1', threshold=0.01, patience=3):\n",
        "\n",
        "    values = history_df[metric].values\n",
        "    if len(values) < patience + 1:\n",
        "        return len(values)\n",
        "\n",
        "    improvements = np.abs(np.diff(values))\n",
        "\n",
        "    for i in range(len(improvements) - (patience - 1)):\n",
        "        if np.all(improvements[i:i+patience] < threshold):\n",
        "            return i + 1\n",
        "    return len(history_df)\n",
        "\n",
        "early_stopping_results = []\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    hist = convergence_histories[name]\n",
        "\n",
        "    final_f1 = hist['val_f1'].iloc[-1]\n",
        "    early_stop_epoch = find_convergence_epoch(\n",
        "        hist, metric='val_f1', threshold=0.01, patience=3\n",
        "    )\n",
        "\n",
        "    auc_f1 = compute_auc_f1(hist, 50)\n",
        "\n",
        "    early_stopping_results.append({\n",
        "        'Optimizer': name,\n",
        "        'Final F1': final_f1,\n",
        "        'F1 @ Epoch 10': hist.loc[9, 'val_f1'],\n",
        "        'F1 @ Epoch 20': hist.loc[19, 'val_f1'],\n",
        "        'Early Stop Epoch': early_stop_epoch,\n",
        "        'AUC F1 (0-50)': auc_f1,\n",
        "        'Time (sec)': convergence_times[name]\n",
        "    })\n",
        "    print(f\"{name:9s}: Early stop @ ep {early_stop_epoch:2d}, AUC={auc_f1:.2f}, Final F1={final_f1:.4f}\")\n",
        "\n",
        "df_early_stop = pd.DataFrame(early_stopping_results)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['Early Stop Epoch'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Epochs to convergence (new metric)', fontsize=11)\n",
        "ax.set_title('Early Stopping Comparison', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax = axes[1]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['AUC F1 (0-50)'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Area Under F1 Curve', fontsize=11)\n",
        "ax.set_title('Overall Convergence Quality', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax = axes[2]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['Time (sec)'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=11)\n",
        "ax.set_title('Computational Efficiency', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "o4VjLNeZBdHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lr_multipliers = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "lr_sensitivity_results = []\n",
        "\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    print(f\"Testing {name}...\")\n",
        "    base_lr = BEST_HPARAMS[name]['lr']\n",
        "    mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "    for mult in lr_multipliers:\n",
        "        lr = base_lr * mult\n",
        "        n_features = X_train.shape[1]\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=30, batch_size=32, seed=42)\n",
        "\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "\n",
        "        lr_sensitivity_results.append({\n",
        "            'Optimizer': name,\n",
        "            'LR Multiplier': mult,\n",
        "            'Learning Rate': lr,\n",
        "            'Final Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Final Val Loss': hist_df['val_loss'].iloc[-1]\n",
        "        })\n",
        "\n",
        "    print(f\"  → LR range: {base_lr*0.1:.5f} to {base_lr*5:.5f}\")\n",
        "\n",
        "df_lr_sens = pd.DataFrame(lr_sensitivity_results)\n",
        "\n",
        "lr_range_df = (\n",
        "    df_lr_sens.groupby('Optimizer')['Final Val F1']\n",
        "    .agg(['max', 'min'])\n",
        ")\n",
        "lr_range_df['range'] = lr_range_df['max'] - lr_range_df['min']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_lr_sens[df_lr_sens['Optimizer'] == name]\n",
        "    ax.plot(data['LR Multiplier'], data['Final Val F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Learning Rate Multiplier (× optimal)', fontsize=11)\n",
        "ax.set_ylabel('Final Validation F1', fontsize=11)\n",
        "ax.set_title('Learning Rate Sensitivity: F1 Score', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xticks(lr_multipliers)\n",
        "ax.set_xticklabels(['0.1×', '0.5×', '1×', '2×', '5×'])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_lr_sens[df_lr_sens['Optimizer'] == name]\n",
        "    ax.plot(data['LR Multiplier'], data['Final Val Loss'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Learning Rate Multiplier (× optimal)', fontsize=11)\n",
        "ax.set_ylabel('Final Validation Loss', fontsize=11)\n",
        "ax.set_title('Learning Rate Sensitivity: Loss', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xticks(lr_multipliers)\n",
        "ax.set_xticklabels(['0.1×', '0.5×', '1×', '2×', '5×'])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ycf7hjgABdEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_label_noise(y, noise_rate=0.15, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    y_noisy = y.copy()\n",
        "    n_flip = int(len(y) * noise_rate)\n",
        "    flip_idx = np.random.choice(len(y), n_flip, replace=False)\n",
        "    y_noisy[flip_idx] = 1 - y_noisy[flip_idx]\n",
        "    return y_noisy\n",
        "\n",
        "noise_rates = [0.0, 0.10, 0.20]\n",
        "noise_results = []\n",
        "\n",
        "for noise_rate in noise_rates:\n",
        "    print(f\"\\nTesting with {noise_rate*100:.0f}% label noise...\")\n",
        "    y_train_noisy = add_label_noise(y_train, noise_rate, seed=42)\n",
        "\n",
        "    for name in OPTIMIZER_NAMES:\n",
        "        n_features = X_train.shape[1]\n",
        "        lr = BEST_HPARAMS[name]['lr']\n",
        "        mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=40, batch_size=32, seed=42)\n",
        "\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train_noisy, X_val, y_val, cfg)\n",
        "        test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "        noise_results.append({\n",
        "            'Optimizer': name,\n",
        "            'Noise Rate': noise_rate,\n",
        "            'Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Test F1': test_results['f1'],\n",
        "            'Test Accuracy': test_results['accuracy']\n",
        "        })\n",
        "\n",
        "        print(f\"  {name:9s}: Val F1={hist_df['val_f1'].iloc[-1]:.4f}, Test F1={test_results['f1']:.4f}\")\n",
        "\n",
        "df_noise = pd.DataFrame(noise_results)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_noise[df_noise['Optimizer'] == name]\n",
        "    ax.plot(data['Noise Rate']*100, data['Test F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Label Noise Rate (%)', fontsize=11)\n",
        "ax.set_ylabel('Test F1 Score', fontsize=11)\n",
        "ax.set_title('Robustness to Label Noise', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_noise[df_noise['Optimizer'] == name]\n",
        "    baseline_f1 = data[data['Noise Rate'] == 0.0]['Test F1'].values[0]\n",
        "    degradation = [(baseline_f1 - row['Test F1']) for _, row in data.iterrows()]\n",
        "    ax.plot(data['Noise Rate']*100, degradation,\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Label Noise Rate (%)', fontsize=11)\n",
        "ax.set_ylabel('F1 Score Degradation', fontsize=11)\n",
        "ax.set_title('Performance Degradation vs Noise', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FBcdHGMjBdBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_sizes = [16, 32, 64, 128, 256]\n",
        "batch_results = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"\\nTesting with batch size {bs}...\")\n",
        "    for name in OPTIMIZER_NAMES:\n",
        "        n_features = X_train.shape[1]\n",
        "        lr = BEST_HPARAMS[name]['lr']\n",
        "        mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=40, batch_size=bs, seed=42)\n",
        "\n",
        "        start = pd.Timestamp.now()\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
        "\n",
        "        test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "        batch_results.append({\n",
        "            'Optimizer': name,\n",
        "            'Batch Size': bs,\n",
        "            'Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Test F1': test_results['f1'],\n",
        "            'Time (sec)': elapsed\n",
        "        })\n",
        "\n",
        "        print(f\"  {name:9s}: Test F1={test_results['f1']:.4f}, Time={elapsed:.2f}s\")\n",
        "\n",
        "df_batch = pd.DataFrame(batch_results)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_batch[df_batch['Optimizer'] == name]\n",
        "    ax.plot(data['Batch Size'], data['Test F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Batch Size', fontsize=11)\n",
        "ax.set_ylabel('Test F1 Score', fontsize=11)\n",
        "ax.set_title('Batch Size Sensitivity: F1 Score', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log', base=2)\n",
        "ax.set_xticks(batch_sizes)\n",
        "ax.set_xticklabels([str(bs) for bs in batch_sizes])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_batch[df_batch['Optimizer'] == name]\n",
        "    ax.plot(data['Batch Size'], data['Time (sec)'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Batch Size', fontsize=11)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=11)\n",
        "ax.set_title('Batch Size Sensitivity: Time', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log', base=2)\n",
        "ax.set_xticks(batch_sizes)\n",
        "ax.set_xticklabels([str(bs) for bs in batch_sizes])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "AyEeFEBiBc_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pd45DTajBc8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFJiYZ70Bc5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XfLLYQUTBc3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUuyWQnKBc0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3MD22Y2BcyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lSZ-TFz7Bcvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ovy5v0uwBctP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyQrpQq4Bcqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fpyoGBUmBcn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rVTCcuXBclS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61xMZtvXBci5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stXNkiApBcgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnqTPV1ABcVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}