{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "np.set_printoptions(suppress=True)\n"
      ],
      "metadata": {
        "id": "t0gEwqRDzvHT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv('diabetes_noisy.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'diabetes_noisy.csv' not found.\")\n",
        "    raise\n",
        "\n",
        "df = df.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "KT9Gzqzp9_pB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = df['Outcome'].values\n",
        "X_df = df.drop(columns=['Outcome'])\n",
        "\n",
        "\n",
        "numeric_cols = X_df.columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_cols)\n",
        "])\n",
        "\n",
        "\n",
        "X = preprocessor.fit_transform(X_df).astype(np.float64)\n",
        "\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(f\"Positive rate: {y.mean():.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFyx8EAj-J5a",
        "outputId": "fa75f07c-5760-4f58-f349-4b42f0f3e80b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (537, 8), Val: (115, 8), Test: (116, 8)\n",
            "Positive rate: 0.3490\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, n_features):\n",
        "        self.weights = np.zeros(n_features, dtype=np.float64)\n",
        "        self.bias = 0.0\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.sigmoid(X @ self.weights + self.bias)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        eps = 1e-15\n",
        "        return -np.mean(y_true*np.log(y_pred+eps) + (1-y_true)*np.log(1-y_pred+eps))\n",
        "\n",
        "    def compute_gradients(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]\n",
        "        err = y_pred - y_true\n",
        "        dw = (X.T @ err)/m\n",
        "        db = np.mean(err)\n",
        "        return dw, db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_prob = self.forward(X)\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        return {\n",
        "            'loss': self.compute_loss(y_true, y_prob),\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_true, y_prob),\n",
        "        }"
      ],
      "metadata": {
        "id": "sLobrNJG-QRL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.lr = learning_rate\n",
        "    def update(self, w, b, dw, db):\n",
        "        return w - self.lr*dw, b - self.lr*db\n",
        "\n",
        "class MomentumOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.m = momentum\n",
        "        self.vw = None\n",
        "        self.vb = 0.0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.vw is None:\n",
        "            self.vw = np.zeros_like(w)\n",
        "        self.vw = self.m*self.vw - self.lr*dw\n",
        "        self.vb = self.m*self.vb - self.lr*db\n",
        "        return w + self.vw, b + self.vb\n",
        "\n",
        "class NesterovOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.lr = learning_rate\n",
        "        self.m = momentum\n",
        "        self.vw = None\n",
        "        self.vb = 0.0\n",
        "\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.vw is None:\n",
        "            self.vw = np.zeros_like(w)\n",
        "\n",
        "        self.vw = self.m * self.vw - self.lr * dw\n",
        "        self.vb = self.m * self.vb - self.lr * db\n",
        "\n",
        "        w_next = w + (self.m * self.vw - self.lr * dw)\n",
        "        b_next = b + (self.m * self.vb - self.lr * db)\n",
        "\n",
        "        return w_next, b_next\n",
        "\n",
        "class AdagradOptimizer:\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.eps = epsilon\n",
        "        self.gs_w = None\n",
        "        self.gs_b = 0.0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.gs_w is None:\n",
        "            self.gs_w = np.zeros_like(w)\n",
        "        self.gs_w += dw**2\n",
        "        self.gs_b += db**2\n",
        "        return (w - self.lr*dw/(np.sqrt(self.gs_w)+self.eps),\n",
        "                b - self.lr*db/(np.sqrt(self.gs_b)+self.eps))\n",
        "\n",
        "class RMSpropOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, decay=0.9, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.decay = decay\n",
        "        self.eps = epsilon\n",
        "        self.ms_w = None\n",
        "        self.ms_b = 0.0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.ms_w is None:\n",
        "            self.ms_w = np.zeros_like(w)\n",
        "        self.ms_w = self.decay*self.ms_w + (1-self.decay)*(dw**2)\n",
        "        self.ms_b = self.decay*self.ms_b + (1-self.decay)*(db**2)\n",
        "        return (w - self.lr*dw/(np.sqrt(self.ms_w)+self.eps),\n",
        "                b - self.lr*db/(np.sqrt(self.ms_b)+self.eps))\n",
        "\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = learning_rate\n",
        "        self.b1 = beta1\n",
        "        self.b2 = beta2\n",
        "        self.eps = epsilon\n",
        "        self.mw = None\n",
        "        self.vw = None\n",
        "        self.mb = 0.0\n",
        "        self.vb = 0.0\n",
        "        self.t = 0\n",
        "    def update(self, w, b, dw, db):\n",
        "        if self.mw is None:\n",
        "            self.mw = np.zeros_like(w)\n",
        "            self.vw = np.zeros_like(w)\n",
        "        self.t += 1\n",
        "        self.mw = self.b1*self.mw + (1-self.b1)*dw\n",
        "        self.mb = self.b1*self.mb + (1-self.b1)*db\n",
        "        self.vw = self.b2*self.vw + (1-self.b2)*(dw**2)\n",
        "        self.vb = self.b2*self.vb + (1-self.b2)*(db**2)\n",
        "        mw_hat = self.mw/(1 - self.b1**self.t)\n",
        "        mb_hat = self.mb/(1 - self.b1**self.t)\n",
        "        vw_hat = self.vw/(1 - self.b2**self.t)\n",
        "        vb_hat = self.vb/(1 - self.b2**self.t)\n",
        "        return (w - self.lr*mw_hat/(np.sqrt(vw_hat)+self.eps),\n",
        "                b - self.lr*mb_hat/(np.sqrt(vb_hat)+self.eps))\n"
      ],
      "metadata": {
        "id": "TFhd7-tJ-XCa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 50\n",
        "    batch_size: int = 32\n",
        "    seed: int = 42\n",
        "    l2_lambda: float = 0.0\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def train_epoch(model, X_tr, y_tr, optimizer, cfg: TrainConfig, track_gradients=False):\n",
        "    n = X_tr.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    total_loss = 0.0\n",
        "    grad_norms = []\n",
        "    bs = cfg.batch_size\n",
        "\n",
        "    for i in range(0, n, bs):\n",
        "        xb = X_tr[idx[i:i+bs]]\n",
        "        yb = y_tr[idx[i:i+bs]]\n",
        "        yp = model.forward(xb)\n",
        "        base_loss = model.compute_loss(yb, yp)\n",
        "        l2_pen = 0.5 * cfg.l2_lambda * np.dot(model.weights, model.weights)\n",
        "        total_loss += (base_loss + l2_pen) * len(yb)\n",
        "\n",
        "        dw, db = model.compute_gradients(xb, yb, yp)\n",
        "        if cfg.l2_lambda > 0.0:\n",
        "            dw = dw + cfg.l2_lambda * model.weights\n",
        "\n",
        "        if track_gradients:\n",
        "            grad_norms.append(np.linalg.norm(dw))\n",
        "\n",
        "        model.weights, model.bias = optimizer.update(model.weights, model.bias, dw, db)\n",
        "\n",
        "    if track_gradients:\n",
        "        return total_loss / n, np.mean(grad_norms)\n",
        "    return total_loss / n\n",
        "\n",
        "def train_model_detailed(model, optimizer, X_tr, y_tr, X_va, y_va, cfg: TrainConfig):\n",
        "    set_seed(cfg.seed)\n",
        "    history = {\n",
        "        'epoch': [], 'train_loss': [], 'val_loss': [],\n",
        "        'val_acc': [], 'val_f1': [], 'val_roc_auc': [], 'grad_norm': []\n",
        "    }\n",
        "\n",
        "    for e in range(cfg.epochs):\n",
        "        tr_loss, grad_norm = train_epoch(model, X_tr, y_tr, optimizer, cfg, track_gradients=True)\n",
        "        val = model.evaluate(X_va, y_va)\n",
        "\n",
        "        history['epoch'].append(e+1)\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['val_loss'].append(val['loss'])\n",
        "        history['val_acc'].append(val['accuracy'])\n",
        "        history['val_f1'].append(val['f1'])\n",
        "        history['val_roc_auc'].append(val['roc_auc'])\n",
        "        history['grad_norm'].append(grad_norm)\n",
        "\n",
        "    return pd.DataFrame(history)"
      ],
      "metadata": {
        "id": "yHZObHat-gjs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_optimizer(name, lr=0.01, momentum=0.9):\n",
        "    if name == 'SGD': return SGDOptimizer(lr)\n",
        "    if name == 'Momentum': return MomentumOptimizer(lr, momentum)\n",
        "    if name == 'Nesterov': return NesterovOptimizer(lr, momentum)\n",
        "    if name == 'Adagrad': return AdagradOptimizer(lr)\n",
        "    if name == 'RMSprop': return RMSpropOptimizer(lr)\n",
        "    if name == 'Adam': return AdamOptimizer(lr)\n",
        "    raise ValueError(name)\n",
        "\n",
        "\n",
        "BEST_HPARAMS = {\n",
        "    'Adam':     {'lr': 0.01,  'momentum': None},\n",
        "    'Momentum': {'lr': 0.01,  'momentum': 0.95},\n",
        "    'Nesterov': {'lr': 0.01,  'momentum': 0.95},\n",
        "    'RMSprop':  {'lr': 0.001, 'momentum': None},\n",
        "    'SGD':      {'lr': 0.1,   'momentum': None},\n",
        "    'Adagrad':  {'lr': 0.03,  'momentum': None},\n",
        "}\n",
        "\n",
        "OPTIMIZER_NAMES = ['Adam', 'Nesterov', 'Momentum', 'RMSprop', 'SGD', 'Adagrad']\n",
        "\n",
        "\n",
        "os.makedirs('pima_results', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "qyv3Hyws-pEM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convergence_histories = {}\n",
        "convergence_times = {}\n",
        "\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    print(f\"Training {name}...\")\n",
        "    n_features = X_train.shape[1]\n",
        "    lr = BEST_HPARAMS[name]['lr']\n",
        "    mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "    model = LogisticRegression(n_features)\n",
        "    opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "    cfg = TrainConfig(epochs=50, batch_size=32, seed=42)\n",
        "\n",
        "    start = pd.Timestamp.now()\n",
        "    hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "    elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
        "\n",
        "    convergence_histories[name] = hist_df\n",
        "    convergence_times[name] = elapsed\n",
        "\n",
        "    test_results = model.evaluate(X_test, y_test)\n",
        "    print(f\"  → F1 @ ep10: {hist_df.loc[9,'val_f1']:.4f}, ep30: {hist_df.loc[29,'val_f1']:.4f}, \"\n",
        "          f\"ep50: {hist_df.loc[49,'val_f1']:.4f} | Test F1: {test_results['f1']:.4f} | Time: {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "colors = plt.cm.tab10(range(len(OPTIMIZER_NAMES)))\n",
        "\n",
        "\n",
        "ax = axes[0, 0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['train_loss'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Training Loss', fontsize=11)\n",
        "ax.set_title('Training Loss Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[0, 1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['val_loss'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Validation Loss', fontsize=11)\n",
        "ax.set_title('Validation Loss Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[1, 0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['val_f1'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Validation F1 Score', fontsize=11)\n",
        "ax.set_title('Validation F1 Convergence', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[1, 1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    ax.plot(convergence_histories[name]['epoch'],\n",
        "            convergence_histories[name]['grad_norm'],\n",
        "            label=name, linewidth=2.5, color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Epoch', fontsize=11)\n",
        "ax.set_ylabel('Gradient Norm', fontsize=11)\n",
        "ax.set_title('Gradient Stability', fontsize=12, fontweight='bold')\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pima_results/1_convergence_analysis.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    convergence_histories[name].to_csv(\n",
        "        f'pima_results/convergence_{name}.csv', index=False\n",
        "    )\n",
        "\n",
        "print(\"\\n Convergence analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuA6847b_SOx",
        "outputId": "1fd58ad0-e61e-48ac-8461-6daec0e541a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Adam...\n",
            "  → F1 @ ep10: 0.5634, ep30: 0.5352, ep50: 0.5217 | Test F1: 0.5714 | Time: 0.57s\n",
            "Training Nesterov...\n",
            "  → F1 @ ep10: 0.5556, ep30: 0.5352, ep50: 0.5217 | Test F1: 0.5714 | Time: 0.55s\n",
            "Training Momentum...\n",
            "  → F1 @ ep10: 0.5556, ep30: 0.5352, ep50: 0.5217 | Test F1: 0.5806 | Time: 0.51s\n",
            "Training RMSprop...\n",
            "  → F1 @ ep10: 0.5263, ep30: 0.5676, ep50: 0.5676 | Test F1: 0.5538 | Time: 0.54s\n",
            "Training SGD...\n",
            "  → F1 @ ep10: 0.5429, ep30: 0.5217, ep50: 0.5217 | Test F1: 0.5625 | Time: 0.65s\n",
            "Training Adagrad...\n",
            "  → F1 @ ep10: 0.5867, ep30: 0.5429, ep50: 0.5429 | Test F1: 0.5846 | Time: 0.53s\n",
            "\n",
            "✓ Convergence analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_auc_f1(history_df, max_epoch=50):\n",
        "\n",
        "    return np.trapz(history_df['val_f1'][:max_epoch], dx=1)\n",
        "\n",
        "early_stopping_results = []\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    hist = convergence_histories[name]\n",
        "\n",
        "\n",
        "    final_f1 = hist['val_f1'].iloc[-1]\n",
        "    target_f1 = 0.95 * final_f1\n",
        "\n",
        "    qualified_epochs = hist[hist['val_f1'] >= target_f1]['epoch']\n",
        "    early_stop_epoch = qualified_epochs.iloc[0] if not qualified_epochs.empty else 50\n",
        "\n",
        "\n",
        "    auc_f1 = compute_auc_f1(hist, 50)\n",
        "\n",
        "    early_stopping_results.append({\n",
        "        'Optimizer': name,\n",
        "        'Final F1': final_f1,\n",
        "        'F1 @ Epoch 10': hist.loc[9, 'val_f1'],\n",
        "        'F1 @ Epoch 20': hist.loc[19, 'val_f1'],\n",
        "        'Early Stop Epoch': early_stop_epoch,\n",
        "        'AUC F1 (0-50)': auc_f1,\n",
        "        'Time (sec)': convergence_times[name]\n",
        "    })\n",
        "    print(f\"{name:9s}: Early stop @ ep {early_stop_epoch:2d}, AUC={auc_f1:.2f}, Final F1={final_f1:.4f}\")\n",
        "\n",
        "df_early_stop = pd.DataFrame(early_stopping_results)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['Early Stop Epoch'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Epochs to 95% of Final F1', fontsize=11)\n",
        "ax.set_title('Early Stopping Comparison', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "\n",
        "ax = axes[1]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['AUC F1 (0-50)'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Area Under F1 Curve', fontsize=11)\n",
        "ax.set_title('Overall Convergence Quality', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "\n",
        "ax = axes[2]\n",
        "bars = ax.bar(df_early_stop['Optimizer'], df_early_stop['Time (sec)'],\n",
        "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=11)\n",
        "ax.set_title('Computational Efficiency', fontsize=12, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pima_results/2_early_stopping_analysis.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "df_early_stop.to_csv('pima_results/early_stopping_results.csv', index=False)\n",
        "print(\"\\n Early stopping analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plPBSv60_ZuB",
        "outputId": "1e538043-01e8-49df-8257-5ef9283d927b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4031696574.py:3: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  return np.trapz(history_df['val_f1'][:max_epoch], dx=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam     : Early stop @ ep  1, AUC=26.18, Final F1=0.5217\n",
            "Nesterov : Early stop @ ep  1, AUC=26.13, Final F1=0.5217\n",
            "Momentum : Early stop @ ep  1, AUC=26.14, Final F1=0.5217\n",
            "RMSprop  : Early stop @ ep  1, AUC=27.64, Final F1=0.5676\n",
            "SGD      : Early stop @ ep  1, AUC=26.43, Final F1=0.5217\n",
            "Adagrad  : Early stop @ ep  1, AUC=27.35, Final F1=0.5429\n",
            "\n",
            "✓ Early stopping analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_multipliers = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "lr_sensitivity_results = []\n",
        "\n",
        "for name in OPTIMIZER_NAMES:\n",
        "    print(f\"Testing {name}...\")\n",
        "    base_lr = BEST_HPARAMS[name]['lr']\n",
        "    mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "    for mult in lr_multipliers:\n",
        "        lr = base_lr * mult\n",
        "        n_features = X_train.shape[1]\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=30, batch_size=32, seed=42)\n",
        "\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "\n",
        "        lr_sensitivity_results.append({\n",
        "            'Optimizer': name,\n",
        "            'LR Multiplier': mult,\n",
        "            'Learning Rate': lr,\n",
        "            'Final Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Final Val Loss': hist_df['val_loss'].iloc[-1]\n",
        "        })\n",
        "\n",
        "    print(f\"  → LR range: {base_lr*0.1:.5f} to {base_lr*5:.5f}\")\n",
        "\n",
        "df_lr_sens = pd.DataFrame(lr_sensitivity_results)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_lr_sens[df_lr_sens['Optimizer'] == name]\n",
        "    ax.plot(data['LR Multiplier'], data['Final Val F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Learning Rate Multiplier (× optimal)', fontsize=11)\n",
        "ax.set_ylabel('Final Validation F1', fontsize=11)\n",
        "ax.set_title('Learning Rate Sensitivity: F1 Score', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xticks(lr_multipliers)\n",
        "ax.set_xticklabels(['0.1×', '0.5×', '1×', '2×', '5×'])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_lr_sens[df_lr_sens['Optimizer'] == name]\n",
        "    ax.plot(data['LR Multiplier'], data['Final Val Loss'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Learning Rate Multiplier (× optimal)', fontsize=11)\n",
        "ax.set_ylabel('Final Validation Loss', fontsize=11)\n",
        "ax.set_title('Learning Rate Sensitivity: Loss', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.set_xticks(lr_multipliers)\n",
        "ax.set_xticklabels(['0.1×', '0.5×', '1×', '2×', '5×'])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pima_results/3_lr_sensitivity.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "df_lr_sens.to_csv('pima_results/lr_sensitivity_results.csv', index=False)\n",
        "print(\"\\n Learning rate sensitivity analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZCcUd7A_mUH",
        "outputId": "8398fd94-76a7-40b6-d97b-b7535cd65288"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Adam...\n",
            "  → LR range: 0.00100 to 0.05000\n",
            "Testing Nesterov...\n",
            "  → LR range: 0.00100 to 0.05000\n",
            "Testing Momentum...\n",
            "  → LR range: 0.00100 to 0.05000\n",
            "Testing RMSprop...\n",
            "  → LR range: 0.00010 to 0.00500\n",
            "Testing SGD...\n",
            "  → LR range: 0.01000 to 0.50000\n",
            "Testing Adagrad...\n",
            "  → LR range: 0.00300 to 0.15000\n",
            "\n",
            "✓ Learning rate sensitivity analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_label_noise(y, noise_rate=0.15, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    y_noisy = y.copy()\n",
        "    n_flip = int(len(y) * noise_rate)\n",
        "    flip_idx = np.random.choice(len(y), n_flip, replace=False)\n",
        "    y_noisy[flip_idx] = 1 - y_noisy[flip_idx]\n",
        "    return y_noisy\n",
        "\n",
        "noise_rates = [0.0, 0.10, 0.20]\n",
        "noise_results = []\n",
        "\n",
        "for noise_rate in noise_rates:\n",
        "    print(f\"\\nTesting with {noise_rate*100:.0f}% label noise...\")\n",
        "    y_train_noisy = add_label_noise(y_train, noise_rate, seed=42)\n",
        "\n",
        "    for name in OPTIMIZER_NAMES:\n",
        "        n_features = X_train.shape[1]\n",
        "        lr = BEST_HPARAMS[name]['lr']\n",
        "        mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=40, batch_size=32, seed=42)\n",
        "\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train_noisy, X_val, y_val, cfg)\n",
        "        test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "        noise_results.append({\n",
        "            'Optimizer': name,\n",
        "            'Noise Rate': noise_rate,\n",
        "            'Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Test F1': test_results['f1'],\n",
        "            'Test Accuracy': test_results['accuracy']\n",
        "        })\n",
        "\n",
        "        print(f\"  {name:9s}: Val F1={hist_df['val_f1'].iloc[-1]:.4f}, Test F1={test_results['f1']:.4f}\")\n",
        "\n",
        "df_noise = pd.DataFrame(noise_results)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_noise[df_noise['Optimizer'] == name]\n",
        "    ax.plot(data['Noise Rate']*100, data['Test F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Label Noise Rate (%)', fontsize=11)\n",
        "ax.set_ylabel('Test F1 Score', fontsize=11)\n",
        "ax.set_title('Robustness to Label Noise', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_noise[df_noise['Optimizer'] == name]\n",
        "    baseline_f1 = data[data['Noise Rate'] == 0.0]['Test F1'].values[0]\n",
        "    degradation = [(baseline_f1 - row['Test F1']) for _, row in data.iterrows()]\n",
        "    ax.plot(data['Noise Rate']*100, degradation,\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Label Noise Rate (%)', fontsize=11)\n",
        "ax.set_ylabel('F1 Score Degradation', fontsize=11)\n",
        "ax.set_title('Performance Degradation vs Noise', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pima_results/4_noise_robustness.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "df_noise.to_csv('pima_results/noise_robustness_results.csv', index=False)\n",
        "print(\"\\n Noise robustness analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua1Y4RUF_1Hh",
        "outputId": "d67fcf72-a68c-4ef2-ba11-3f6bbd06d2da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with 0% label noise...\n",
            "  Adam     : Val F1=0.5429, Test F1=0.5625\n",
            "  Nesterov : Val F1=0.5634, Test F1=0.5625\n",
            "  Momentum : Val F1=0.5634, Test F1=0.5625\n",
            "  RMSprop  : Val F1=0.5867, Test F1=0.5588\n",
            "  SGD      : Val F1=0.5217, Test F1=0.5806\n",
            "  Adagrad  : Val F1=0.5429, Test F1=0.5625\n",
            "\n",
            "Testing with 10% label noise...\n",
            "  Adam     : Val F1=0.5278, Test F1=0.5758\n",
            "  Nesterov : Val F1=0.5278, Test F1=0.5538\n",
            "  Momentum : Val F1=0.5278, Test F1=0.5538\n",
            "  RMSprop  : Val F1=0.5600, Test F1=0.6176\n",
            "  SGD      : Val F1=0.5278, Test F1=0.5538\n",
            "  Adagrad  : Val F1=0.5070, Test F1=0.5538\n",
            "\n",
            "Testing with 20% label noise...\n",
            "  Adam     : Val F1=0.5000, Test F1=0.5846\n",
            "  Nesterov : Val F1=0.5000, Test F1=0.5846\n",
            "  Momentum : Val F1=0.5000, Test F1=0.5846\n",
            "  RMSprop  : Val F1=0.4789, Test F1=0.6087\n",
            "  SGD      : Val F1=0.5000, Test F1=0.5625\n",
            "  Adagrad  : Val F1=0.4789, Test F1=0.5758\n",
            "\n",
            "✓ Noise robustness analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [16, 32, 64, 128, 256]\n",
        "batch_results = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"\\nTesting with batch size {bs}...\")\n",
        "    for name in OPTIMIZER_NAMES:\n",
        "        n_features = X_train.shape[1]\n",
        "        lr = BEST_HPARAMS[name]['lr']\n",
        "        mom = BEST_HPARAMS[name]['momentum']\n",
        "\n",
        "        model = LogisticRegression(n_features)\n",
        "        opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "        cfg = TrainConfig(epochs=40, batch_size=bs, seed=42)\n",
        "\n",
        "        start = pd.Timestamp.now()\n",
        "        hist_df = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "        elapsed = (pd.Timestamp.now() - start).total_seconds()\n",
        "\n",
        "        test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "        batch_results.append({\n",
        "            'Optimizer': name,\n",
        "            'Batch Size': bs,\n",
        "            'Val F1': hist_df['val_f1'].iloc[-1],\n",
        "            'Test F1': test_results['f1'],\n",
        "            'Time (sec)': elapsed\n",
        "        })\n",
        "\n",
        "        print(f\"  {name:9s}: Test F1={test_results['f1']:.4f}, Time={elapsed:.2f}s\")\n",
        "\n",
        "df_batch = pd.DataFrame(batch_results)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "\n",
        "ax = axes[0]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_batch[df_batch['Optimizer'] == name]\n",
        "    ax.plot(data['Batch Size'], data['Test F1'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Batch Size', fontsize=11)\n",
        "ax.set_ylabel('Test F1 Score', fontsize=11)\n",
        "ax.set_title('Batch Size Sensitivity: F1 Score', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log', base=2)\n",
        "ax.set_xticks(batch_sizes)\n",
        "ax.set_xticklabels([str(bs) for bs in batch_sizes])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "ax = axes[1]\n",
        "for i, name in enumerate(OPTIMIZER_NAMES):\n",
        "    data = df_batch[df_batch['Optimizer'] == name]\n",
        "    ax.plot(data['Batch Size'], data['Time (sec)'],\n",
        "            marker='o', label=name, linewidth=2.5, markersize=8,\n",
        "            color=colors[i], alpha=0.8)\n",
        "ax.set_xlabel('Batch Size', fontsize=11)\n",
        "ax.set_ylabel('Training Time (seconds)', fontsize=11)\n",
        "ax.set_title('Batch Size Sensitivity: Time', fontsize=12, fontweight='bold')\n",
        "ax.set_xscale('log', base=2)\n",
        "ax.set_xticks(batch_sizes)\n",
        "ax.set_xticklabels([str(bs) for bs in batch_sizes])\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pima_results/5_batch_sensitivity.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "df_batch.to_csv('pima_results/batch_sensitivity_results.csv', index=False)\n",
        "print(\"\\n Batch size sensitivity analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oi2oKAXAsF8",
        "outputId": "97f1fef3-ab62-4059-8281-099a01d5da0c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with batch size 16...\n",
            "  Adam     : Test F1=0.5625, Time=0.48s\n",
            "  Nesterov : Test F1=0.5714, Time=0.45s\n",
            "  Momentum : Test F1=0.5625, Time=0.48s\n",
            "  RMSprop  : Test F1=0.5625, Time=0.46s\n",
            "  SGD      : Test F1=0.5806, Time=0.44s\n",
            "  Adagrad  : Test F1=0.5938, Time=0.43s\n",
            "\n",
            "Testing with batch size 32...\n",
            "  Adam     : Test F1=0.5625, Time=0.43s\n",
            "  Nesterov : Test F1=0.5625, Time=0.40s\n",
            "  Momentum : Test F1=0.5625, Time=0.40s\n",
            "  RMSprop  : Test F1=0.5588, Time=0.40s\n",
            "  SGD      : Test F1=0.5806, Time=0.38s\n",
            "  Adagrad  : Test F1=0.5625, Time=0.41s\n",
            "\n",
            "Testing with batch size 64...\n",
            "  Adam     : Test F1=0.5625, Time=0.35s\n",
            "  Nesterov : Test F1=0.5625, Time=0.54s\n",
            "  Momentum : Test F1=0.5625, Time=0.52s\n",
            "  RMSprop  : Test F1=0.5217, Time=0.52s\n",
            "  SGD      : Test F1=0.5806, Time=0.58s\n",
            "  Adagrad  : Test F1=0.5312, Time=0.54s\n",
            "\n",
            "Testing with batch size 128...\n",
            "  Adam     : Test F1=0.5846, Time=0.36s\n",
            "  Nesterov : Test F1=0.5714, Time=0.34s\n",
            "  Momentum : Test F1=0.5714, Time=0.34s\n",
            "  RMSprop  : Test F1=0.5634, Time=0.35s\n",
            "  SGD      : Test F1=0.5938, Time=0.34s\n",
            "  Adagrad  : Test F1=0.5231, Time=0.34s\n",
            "\n",
            "Testing with batch size 256...\n",
            "  Adam     : Test F1=0.5538, Time=0.36s\n",
            "  Nesterov : Test F1=0.5714, Time=0.34s\n",
            "  Momentum : Test F1=0.5714, Time=0.33s\n",
            "  RMSprop  : Test F1=0.6027, Time=0.34s\n",
            "  SGD      : Test F1=0.5758, Time=0.33s\n",
            "  Adagrad  : Test F1=0.5075, Time=0.34s\n",
            "\n",
            "✓ Batch size sensitivity analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_data = []\n",
        "for name in OPTIMIZER_NAMES:\n",
        "\n",
        "    hist = convergence_histories[name]\n",
        "    n_features = X_train.shape[1]\n",
        "    lr = BEST_HPARAMS[name]['lr']\n",
        "    mom = BEST_HPARAMS[name]['momentum']\n",
        "    model = LogisticRegression(n_features)\n",
        "    opt = make_optimizer(name, lr=lr, momentum=(mom if mom is not None else 0.9))\n",
        "    cfg = TrainConfig(epochs=50, batch_size=32, seed=42)\n",
        "\n",
        "\n",
        "    _ = train_model_detailed(model, opt, X_train, y_train, X_val, y_val, cfg)\n",
        "    test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "    early_stop = df_early_stop[df_early_stop['Optimizer'] == name]['Early Stop Epoch'].values[0]\n",
        "    auc_f1 = df_early_stop[df_early_stop['Optimizer'] == name]['AUC F1 (0-50)'].values[0]\n",
        "    time_taken = convergence_times[name]\n",
        "\n",
        "\n",
        "    lr_std = df_lr_sens[df_lr_sens['Optimizer'] == name]['Final Val F1'].std()\n",
        "\n",
        "\n",
        "    noise_data = df_noise[df_noise['Optimizer'] == name]\n",
        "    noise_drop = noise_data[noise_data['Noise Rate'] == 0.0]['Test F1'].values[0] - \\\n",
        "                 noise_data[noise_data['Noise Rate'] == 0.20]['Test F1'].values[0]\n",
        "\n",
        "    summary_data.append({\n",
        "        'Optimizer': name,\n",
        "        'Test F1': test_results['f1'],\n",
        "        'Test Accuracy': test_results['accuracy'],\n",
        "        'Test ROC-AUC': test_results['roc_auc'],\n",
        "        'Early Stop Epoch': early_stop,\n",
        "        'AUC F1 (Convergence)': auc_f1,\n",
        "        'Time (sec)': time_taken,\n",
        "        'LR Sensitivity (σ)': lr_std,\n",
        "        'Noise Robustness (Δ)': noise_drop\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "df_summary = df_summary.round(4)\n",
        "\n",
        "print(\"\\nFINAL COMPARISON TABLE:\")\n",
        "print(\"=\"*70)\n",
        "print(df_summary.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_summary.to_csv('pima_results/FINAL_SUMMARY.csv', index=False)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "x_pos = np.arange(len(OPTIMIZER_NAMES))\n",
        "width = 0.25\n",
        "ax1.bar(x_pos - width, df_summary['Test F1'], width, label='F1 Score', alpha=0.8)\n",
        "ax1.bar(x_pos, df_summary['Test Accuracy'], width, label='Accuracy', alpha=0.8)\n",
        "ax1.bar(x_pos + width, df_summary['Test ROC-AUC'], width, label='ROC-AUC', alpha=0.8)\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels(OPTIMIZER_NAMES)\n",
        "ax1.set_ylabel('Score', fontsize=11)\n",
        "ax1.set_title('Final Test Performance Comparison', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "bars = ax2.barh(OPTIMIZER_NAMES, df_summary['AUC F1 (Convergence)'], color=colors, alpha=0.7)\n",
        "ax2.set_xlabel('AUC F1 Score', fontsize=10)\n",
        "ax2.set_title('Convergence Quality', fontsize=11, fontweight='bold')\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "bars = ax3.barh(OPTIMIZER_NAMES, df_summary['Time (sec)'], color=colors, alpha=0.7)\n",
        "ax3.set_xlabel('Training Time (sec)', fontsize=10)\n",
        "ax3.set_title('Computational Efficiency', fontsize=11, fontweight='bold')\n",
        "ax3.grid(axis='x', alpha=0.3)\n",
        "\n",
        "\n",
        "ax4 = fig.add_subplot(gs[1, 2])\n",
        "bars = ax4.barh(OPTIMIZER_NAMES, df_summary['Early Stop Epoch'], color=colors, alpha=0.7)\n",
        "ax4.set_xlabel('Epochs to 95% Final F1', fontsize=10)\n",
        "ax4.set_title('Early Stopping Point', fontsize=11, fontweight='bold')\n",
        "ax4.grid(axis='x', alpha=0.3)\n",
        "\n",
        "\n",
        "ax5 = fig.add_subplot(gs[2, 0])\n",
        "bars = ax5.barh(OPTIMIZER_NAMES, df_summary['LR Sensitivity'], color=colors, alpha=0.7)\n",
        "ax5.set_xlabel('Std Dev of F1 (lower=robust)', fontsize=10)\n",
        "ax5.set_title('LR Sensitivity', fontsize=11, fontweight='bold')\n",
        "ax5.grid(axis='x', alpha=0.3)\n",
        "\n",
        "\n",
        "ax6 = fig.add_subplot(gs[2, 1])\n",
        "bars = ax6.barh(OPTIMIZER_NAMES, df_summary['Noise Robustness'], color=colors, alpha=0.7)\n",
        "ax6.set_xlabel('F1 Drop at 20% noise (lower=robust)', fontsize=10)\n",
        "ax6.set_title('Noise Robustness', fontsize=11, fontweight='bold')\n",
        "ax6.grid(axis='x', alpha=0.3)\n",
        "\n",
        "\n",
        "ax7 = fig.add_subplot(gs[2, 2])\n",
        "\n",
        "composite = (\n",
        "    (df_summary['Test F1'] - df_summary['Test F1'].min()) / (df_summary['Test F1'].max() - df_summary['Test F1'].min()) * 0.3 +\n",
        "    (df_summary['AUC F1 (Convergence)'] - df_summary['AUC F1 (Convergence)'].min()) / (df_summary['AUC F1 (Convergence)'].max() - df_summary['AUC F1 (Convergence)'].min()) * 0.25 +\n",
        "    (1 - (df_summary['Time (sec)'] - df_summary['Time (sec)'].min()) / (df_summary['Time (sec)'].max() - df_summary['Time (sec)'].min())) * 0.15 +\n",
        "    (1 - (df_summary['LR Sensitivity '] - df_summary['LR Sensitivity '].min()) / (df_summary['LR Sensitivity (σ)'].max() - df_summary['LR Sensitivity (σ)'].min())) * 0.15 +\n",
        "    (1 - (df_summary['Noise Robustness '] - df_summary['Noise Robustness '].min()) / (df_summary['Noise Robustness (Δ)'].max() - df_summary['Noise Robustness (Δ)'].min())) * 0.15\n",
        ")\n",
        "df_summary['Composite Score'] = composite.fillna(0)\n",
        "df_ranked = df_summary.sort_values('Composite Score', ascending=False)\n",
        "\n",
        "bars = ax7.barh(df_ranked['Optimizer'], df_ranked['Composite Score'],\n",
        "                color=colors[:len(df_ranked)], alpha=0.7)\n",
        "ax7.set_xlabel('Composite Score', fontsize=10)\n",
        "ax7.set_title('Overall Ranking', fontsize=11, fontweight='bold')\n",
        "ax7.grid(axis='x', alpha=0.3)\n",
        "ax7.invert_yaxis()\n",
        "\n",
        "plt.suptitle('COMPREHENSIVE OPTIMIZER COMPARISON DASHBOARD (PIMA)',\n",
        "             fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.savefig('pima_results/COMPREHENSIVE_DASHBOARD.png', dpi=200, bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0BgeyeGA3qk",
        "outputId": "2413be23-ff06-41c3-9d40-657ce4511b26"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL COMPARISON TABLE:\n",
            "======================================================================\n",
            "Optimizer  Test F1  Test Accuracy  Test ROC-AUC  Early Stop Epoch  AUC F1 (Convergence)  Time (sec)  LR Sensitivity (σ)  Noise Robustness (Δ)\n",
            "     Adam   0.5714         0.7672        0.8257                 1               26.1828      0.5686              0.0137               -0.0221\n",
            " Nesterov   0.5714         0.7672        0.8260                 1               26.1337      0.5534              0.0126               -0.0221\n",
            " Momentum   0.5806         0.7759        0.8257                 1               26.1446      0.5091              0.0122               -0.0221\n",
            "  RMSprop   0.5538         0.7500        0.8283                 1               27.6404      0.5427              0.0105               -0.0499\n",
            "      SGD   0.5625         0.7586        0.8270                 1               26.4299      0.6536              0.0263                0.0181\n",
            "  Adagrad   0.5846         0.7672        0.8306                 1               27.3528      0.5311              0.0162               -0.0133\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRzehygkCIDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}